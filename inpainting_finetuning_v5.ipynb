{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author: Seunghee Kim\n",
    "Created on: 24.12.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import piq\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, StableDiffusionInpaintPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "\n",
    "\n",
    "IMAGE_SIZE = (512, 512)\n",
    "# PROMPT = \"Fill the missing area with the sea background\"\n",
    "PROMPT = 'fill the area with ocean, remove people'\n",
    "SAVE_EXAMPLE_DIR = './masking_examples_v5'\n",
    "os.makedirs(SAVE_EXAMPLE_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 9\n",
    "EPOCHS = 3\n",
    "# LEARNING_RATE = 1e-5\n",
    "LEARNING_RATE = 5e-7\n",
    "TRAIN_RATIO = 0.8\n",
    "VALID_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "assert TRAIN_RATIO + VALID_RATIO + TEST_RATIO == 1.0\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 마스킹 증강 횟수\n",
    "MASK_MULTIPLIER = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드 및 마스킹 함수\n",
    "beach_dataset = load_dataset('louiscklaw/beach_512', split='train', cache_dir='beach_dataset_huggingface')\n",
    "beach_dataset = beach_dataset.select(range(500)) # 500개만 사용\n",
    "\n",
    "def random_mask(image_size, num_shapes, max_shape_size=100):\n",
    "    mask = Image.new(\"L\", image_size, 0) \n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    width, height = image_size\n",
    "\n",
    "    for _ in range(random.randint(1, num_shapes)):\n",
    "        shape_type = random.choice([\"ellipse\", \"rectangle\", \"polygon\"])\n",
    "        \n",
    "        x1, y1 = random.randint(0, width - max_shape_size), random.randint(0, height - max_shape_size)\n",
    "        \n",
    "        x2 = x1 + random.randint(20, max_shape_size)\n",
    "        y2 = y1 + random.randint(20, max_shape_size)\n",
    "        \n",
    "        x2 = min(x2, width)\n",
    "        y2 = min(y2, height)\n",
    "        \n",
    "        if shape_type == \"ellipse\":\n",
    "            draw.ellipse([x1, y1, x2, y2], fill=255)\n",
    "        elif shape_type == \"rectangle\":\n",
    "            draw.rectangle([x1, y1, x2, y2], fill=255)\n",
    "        elif shape_type == \"polygon\":\n",
    "            points = [\n",
    "                (x1, y1),\n",
    "                (x2, y1 + random.randint(0, max_shape_size // 2)),\n",
    "                (x1 + random.randint(0, max_shape_size // 2), y2)\n",
    "            ]\n",
    "            draw.polygon(points, fill=255)\n",
    "    \n",
    "    mask = np.array(mask)\n",
    "    return mask\n",
    "\n",
    "resize_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "])\n",
    "\n",
    "class InpaintingDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, prompt=PROMPT, transform=resize_transform, mask_multiplier=1):\n",
    "        self.dataset = hf_dataset\n",
    "        self.prompt = prompt\n",
    "        self.transform = transform\n",
    "        self.mask_multiplier = mask_multiplier\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * self.mask_multiplier\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx // self.mask_multiplier\n",
    "\n",
    "        data = self.dataset[real_idx]\n",
    "        image = data['image']\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        image = image.convert(\"RGB\")\n",
    "        mask = random_mask(IMAGE_SIZE, num_shapes=5)\n",
    "        mask_pil = Image.fromarray(mask)\n",
    "        image_np = np.array(image)\n",
    "        \n",
    "        masked_image_np = image_np.copy()\n",
    "        masked_image_np[mask == 255] = 255\n",
    "\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        image_t = to_tensor(image)\n",
    "        masked_image_t = to_tensor(Image.fromarray(masked_image_np))\n",
    "        mask_t = to_tensor(mask_pil).float() # [1,H,W]\n",
    "        mask_t = transforms.Resize((64, 64), interpolation=transforms.InterpolationMode.BILINEAR)(mask_t)\n",
    "        \n",
    "        return {\n",
    "            'original_image': image_t,\n",
    "            'masked_image': masked_image_t,\n",
    "            'mask': mask_t,\n",
    "            'prompt': self.prompt\n",
    "        }\n",
    "\n",
    "full_dataset = InpaintingDataset(beach_dataset, mask_multiplier=MASK_MULTIPLIER)\n",
    "\n",
    "# 마스킹 예시 저장\n",
    "for i in range(100):\n",
    "    sample = full_dataset[i]\n",
    "    masked_img = transforms.ToPILImage()(sample['masked_image'])\n",
    "    original_img = transforms.ToPILImage()(sample['original_image'])\n",
    "    mask_img = transforms.ToPILImage()(sample['mask'])\n",
    "    \n",
    "    masked_img.save(os.path.join(SAVE_EXAMPLE_DIR, f\"masked_{i}.png\"))\n",
    "    original_img.save(os.path.join(SAVE_EXAMPLE_DIR, f\"original_{i}.png\"))\n",
    "    mask_img.save(os.path.join(SAVE_EXAMPLE_DIR, f\"mask_{i}.png\"))\n",
    "\n",
    "# Train/Valid/Test Split 및 DataLoader 구축\n",
    "dataset_length = len(full_dataset)\n",
    "train_length = int(dataset_length * TRAIN_RATIO)\n",
    "valid_length = int(dataset_length * VALID_RATIO)\n",
    "test_length = dataset_length - train_length - valid_length\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(full_dataset, [train_length, valid_length, test_length],\n",
    "                                                         generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 모델 파인튜닝\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "    cache_dir='./cache_dir'\n",
    ").to(DEVICE)\n",
    "\n",
    "pipe.unet.to(torch.float32)\n",
    "pipe.text_encoder.to(torch.float32)\n",
    "pipe.vae.to(torch.float32)\n",
    "\n",
    "unet = pipe.unet\n",
    "text_encoder = pipe.text_encoder\n",
    "vae = pipe.vae\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train_epoch():\n",
    "    unet.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            input_ids = tokenizer(batch['prompt'], padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "            text_embeddings = text_encoder(input_ids)[0]\n",
    "            latents = vae.encode(batch['original_image'].to(DEVICE)*2-1).latent_dist.sample() * 0.18215\n",
    "            masked_latents = vae.encode(batch['masked_image'].to(DEVICE)*2-1).latent_dist.sample() * 0.18215\n",
    "            mask = batch['mask'].to(DEVICE)\n",
    "\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, 1000, (latents.shape[0],), device=DEVICE, dtype=torch.long)\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            unet_input = torch.cat([noisy_latents, masked_latents, mask], dim=1)\n",
    "            model_pred = unet(unet_input, timesteps, encoder_hidden_states=text_embeddings, return_dict=False)[0]\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(model_pred, noise)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Warning: NaN loss detected, skipping this batch.\")\n",
    "            continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item() * latents.size(0)\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "\n",
    "def evaluate(loader):\n",
    "    unet.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            with torch.cuda.amp.autocast():\n",
    "                input_ids = tokenizer(batch['prompt'], padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "                text_embeddings = text_encoder(input_ids)[0]\n",
    "                latents = vae.encode(batch['original_image'].to(DEVICE)*2-1).latent_dist.sample() * 0.18215\n",
    "                masked_latents = vae.encode(batch['masked_image'].to(DEVICE)*2-1).latent_dist.sample() * 0.18215\n",
    "                mask = batch['mask'].to(DEVICE)\n",
    "                \n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(0, 1000, (latents.shape[0],), device=DEVICE, dtype=torch.long)\n",
    "                noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                unet_input = torch.cat([noisy_latents, masked_latents, mask], dim=1)\n",
    "                model_pred = unet(unet_input, timesteps, encoder_hidden_states=text_embeddings, return_dict=False)[0]\n",
    "                \n",
    "                loss = torch.nn.functional.mse_loss(model_pred, noise)\n",
    "                val_loss += loss.item() * latents.size(0)\n",
    "    return val_loss / len(loader.dataset)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch()\n",
    "    valid_loss = evaluate(valid_loader)\n",
    "    print(f\"Epoch {epoch}, Train Loss: {train_loss}, Valid Loss: {valid_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSIM, LPIPS Metric Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(loader, pipeline=pipe):\n",
    "    ssim_scores = []\n",
    "    lpips_metric = piq.LPIPS().to(DEVICE)\n",
    "    lpips_scores = []\n",
    "    \n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Computing metrics\"):\n",
    "            batch_size = batch['original_image'].shape[0]\n",
    "            prompts = batch['prompt']\n",
    "            if isinstance(prompts, str):\n",
    "                prompts = [prompts]*batch_size\n",
    "            elif not isinstance(prompts, list):\n",
    "                prompts = [batch['prompt']] * batch_size\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                original_img_t = batch['original_image'][i].to(DEVICE)\n",
    "                mask_t = batch['mask'][i].cpu()\n",
    "                masked_image_t = batch['masked_image'][i].cpu()\n",
    "\n",
    "                masked_image_pil = transforms.ToPILImage()(masked_image_t)\n",
    "                mask_pil = transforms.ToPILImage()(mask_t).convert(\"L\")\n",
    "                \n",
    "                prompt = prompts[i]\n",
    "\n",
    "                result_image = pipeline(prompt=prompt, image=masked_image_pil, mask_image=mask_pil).images[0]\n",
    "                gen_image_t = transforms.ToTensor()(result_image).unsqueeze(0).to(DEVICE)\n",
    "                original_img_t = original_img_t.unsqueeze(0)\n",
    "\n",
    "                ssim_val = piq.ssim(gen_image_t, original_img_t, data_range=1.0)\n",
    "                ssim_scores.append(ssim_val.item())\n",
    "\n",
    "                lpips_val = lpips_metric(gen_image_t, original_img_t)\n",
    "                lpips_scores.append(lpips_val.mean().item())\n",
    "\n",
    "    mean_ssim = np.mean(ssim_scores)\n",
    "    mean_lpips = np.mean(lpips_scores)\n",
    "    return mean_ssim, mean_lpips\n",
    "\n",
    "mean_ssim, mean_lpips = compute_metrics(valid_loader)\n",
    "test_ssim, test_lpips = compute_metrics(test_loader, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Metrics:\")\n",
    "print(\"SSIM:\", mean_ssim)\n",
    "print(\"LPIPS:\", mean_lpips)\n",
    "\n",
    "# os.makedirs('./finetuned_model', exist_ok=True)\n",
    "# pipe.save_pretrained('./finetuned_model')\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(\"SSIM:\", test_ssim)\n",
    "print(\"LPIPS:\", test_lpips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(\"two-people-sea.jpg\").convert(\"RGB\")\n",
    "input_mask = Image.open(\"two-people-sea-mask.png\").convert(\"L\")\n",
    "test_prompt = PROMPT\n",
    "\n",
    "original_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    # \"runwayml/stable-diffusion-inpainting\", \n",
    "    # \"botp/stable-diffusion-v1-5-inpainting\",\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "    cache_dir='./cache_dir'\n",
    "    ).to(DEVICE)\n",
    "original_pipe.unet.to(torch.float32)\n",
    "original_pipe.text_encoder.to(torch.float32)\n",
    "original_pipe.vae.to(torch.float32)\n",
    "\n",
    "def run_inference(pipeline, image, mask, prompt):\n",
    "    with torch.no_grad():\n",
    "        result = pipeline(prompt=prompt, image=image, mask_image=mask).images[0]\n",
    "    return result\n",
    "\n",
    "num_generations = 10\n",
    "original_results = []\n",
    "finetuned_results = []\n",
    "\n",
    "for i in range(num_generations):\n",
    "    # Original 결과 생성\n",
    "    original_result = run_inference(original_pipe, input_image, input_mask, test_prompt)\n",
    "    original_file = f\"original_result_{i+1}.png\"\n",
    "    original_result.save(original_file)\n",
    "    original_results.append(original_result)\n",
    "\n",
    "    # Fine-tuned 결과 생성\n",
    "    finetuned_result = run_inference(pipe, input_image, input_mask, test_prompt)\n",
    "    finetuned_file = f\"finetuned_result_{i+1}.png\"\n",
    "    finetuned_result.save(finetuned_file)\n",
    "    finetuned_results.append(finetuned_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(num_generations):\n",
    "\n",
    "    plt.subplot(2, num_generations, i + 1)\n",
    "    plt.imshow(original_results[i])\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Original {i + 1}')\n",
    "    \n",
    "    plt.subplot(2, num_generations, num_generations + i + 1)\n",
    "    plt.imshow(finetuned_results[i])\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Fine-tuned {i + 1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# epochs = [1, 2, 3]\n",
    "# train_loss = [0.19729973109904678, 0.14593217349145562, 0.13217759937467052]\n",
    "# valid_loss = [0.16069402199983596, 0.1383479192107916, 0.11289855517819523]\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(epochs, train_loss, label='Train Loss', marker='o', linewidth=2)\n",
    "# plt.plot(epochs, valid_loss, label='Validation Loss', marker='s', linewidth=2)\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=14)\n",
    "# plt.ylabel('Loss', fontsize=14)\n",
    "# plt.title('Train and Validation Loss', fontsize=16)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.savefig('train_validation_loss.png', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ksh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
